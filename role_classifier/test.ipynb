{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# read tsv\n",
    "df = pd.read_csv('gamail_orgnaizer_export_1701309047952.tsv', sep='\\t')\n",
    "\n",
    "# make account_role\tall equal to 0\n",
    "df['account_role'] = 0\n",
    "\n",
    "# drop content, account_name\n",
    "data = df.drop(['content', 'account_name', 'mail_labels', 'receiver'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        22\\n\\n    accuracy                           1.00        22\\n   macro avg       1.00      1.00      1.00        22\\nweighted avg       1.00      1.00      1.00        22\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Sample Data\n",
    "\n",
    "data['combined_text'] = data['account_mail_address'] + ' ' + data['sender'] + ' ' + data['subject']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['combined_text'], data['account_role'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline that first converts text data to TF-IDF vectors and then applies Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/jackyzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m pipeline_subject \u001b[39m=\u001b[39m make_pipeline(CountVectorizer(), LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Training the models\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pipeline_mail\u001b[39m.\u001b[39mfit(X_train[\u001b[39m'\u001b[39m\u001b[39maccount_mail_address\u001b[39m\u001b[39m'\u001b[39m], y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pipeline_sender\u001b[39m.\u001b[39mfit(X_train[\u001b[39m'\u001b[39m\u001b[39msender\u001b[39m\u001b[39m'\u001b[39m], y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m pipeline_subject\u001b[39m.\u001b[39mfit(X_train[\u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m], y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1241\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1239\u001b[0m classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m n_classes \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis solver needs samples of at least 2 classes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m in the data, but the data contains only one\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m class: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m         \u001b[39m%\u001b[39m classes_[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1246\u001b[0m     )\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1249\u001b[0m     n_classes \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = data[['account_mail_address', 'sender', 'subject']]\n",
    "y = data['account_role']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# We will use a CountVectorizer to convert text data to numeric format\n",
    "# It's important to note that this is a very basic form of text processing\n",
    "# More complex techniques might be required for better performance\n",
    "\n",
    "# Creating pipelines for each feature\n",
    "pipeline_mail = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n",
    "pipeline_sender = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n",
    "pipeline_subject = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Training the models\n",
    "pipeline_mail.fit(X_train['account_mail_address'], y_train)\n",
    "pipeline_sender.fit(X_train['sender'], y_train)\n",
    "pipeline_subject.fit(X_train['subject'], y_train)\n",
    "\n",
    "# Evaluating the models\n",
    "score_mail = pipeline_mail.score(X_test['account_mail_address'], y_test)\n",
    "score_sender = pipeline_sender.score(X_test['sender'], y_test)\n",
    "score_subject = pipeline_subject.score(X_test['subject'], y_test)\n",
    "\n",
    "score_mail, score_sender, score_subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Obtaining dependency information for torchtext from https://files.pythonhosted.org/packages/f2/21/6934dc4820c2bb12c8ae2608a2456df31a2c0c58973f2278059757f35f8d/torchtext-0.16.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchtext-0.16.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: tqdm in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torchtext) (2.31.0)\n",
      "Collecting torch==2.1.1 (from torchtext)\n",
      "  Obtaining dependency information for torch==2.1.1 from https://files.pythonhosted.org/packages/5b/46/3def5bdaae03c21a7662673e6bda1f60a046afce48e0d6319ce4542bca31/torch-2.1.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torchtext) (1.26.0)\n",
      "Collecting torchdata==0.7.1 (from torchtext)\n",
      "  Obtaining dependency information for torchdata==0.7.1 from https://files.pythonhosted.org/packages/35/b2/7ed3a80ae0673b940f2af14281dc02dee0f667c6094e6dcd399fa35249a7/torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torch==2.1.1->torchtext) (2023.9.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from torchdata==0.7.1->torchtext) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.1.1->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jackyzhang/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.1.1->torchtext) (1.3.0)\n",
      "Downloading torchtext-0.16.1-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp311-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchdata, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "Successfully installed torch-2.1.1 torchdata-0.7.1 torchtext-0.16.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m email, label\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Split the dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(emails, encoded_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# DataLoader\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackyzhang/Documents/GitHub/gmail_organizer/role_classifier/test.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m train_data \u001b[39m=\u001b[39m EmailDataset(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 2]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample data - replace with your data\n",
    "emails = [\"Sample email texts...\"]\n",
    "labels = [\"teacher\", \"student\"]  # Labels\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build Vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(emails), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Dataset Class\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails, labels):\n",
    "        self.emails = emails\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = torch.tensor(vocab(tokenizer(self.emails[idx])), dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return email, label\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# DataLoader\n",
    "train_data = EmailDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Model Initialization\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Training (simplified for example)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for emails, labels in train_loader:\n",
    "        emails, labels = emails.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(emails, [len(email) for email in emails]).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Training Complete\")\n",
    "\n",
    "# Add evaluation, saving/loading model, and inference as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
